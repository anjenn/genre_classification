# rq1_misclassification.py

from rq1_utils import *
from rq1_classical import run_classical_rq1
from rq1_cnn import run_cnn_rq1


def get_confusion_pairs(cm: np.ndarray, class_names: List[str], top_k: int = 5):
    """
    Given a confusion matrix (true x pred, counts),
    returns list of ((true, pred), count) sorted by count (excluding diagonal).
    """
    pairs = []
    for i, true_c in enumerate(class_names):
        for j, pred_c in enumerate(class_names):
            if i == j:
                continue
            count = cm[i, j]
            if count > 0:
                pairs.append(((true_c, pred_c), count))

    pairs = sorted(pairs, key=lambda x: x[1], reverse=True)
    return pairs[:top_k]


def analyze_confusion_pair_features(df: pd.DataFrame,
                                    y_true, y_pred,
                                    true_class: str, pred_class: str,
                                    label_col: str = "label",
                                    feature_cols: List[str] = None,
                                    feature_to_plot: str = None):
    """
    For a given pair (true_class -> pred_class), look at distribution of features.
    If feature_to_plot is given, show histograms.
    """
    if feature_cols is None:
        non_feature_cols = {label_col, "filepath", "track_id", "split"}
        feature_cols = [c for c in df.columns if c not in non_feature_cols]

    mask_pair = (y_true == true_class) & (y_pred == pred_class)
    pair_idx = np.where(mask_pair)[0]

    print(f"[PAIR] {true_class} → {pred_class}: {len(pair_idx)} misclassifications")

    if feature_to_plot is not None and feature_to_plot in feature_cols and len(pair_idx) > 0:
        vals_mis = df.iloc[pair_idx][feature_to_plot].values

        # Compare with all correctly predicted of true_class
        mask_correct_true = (y_true == true_class) & (y_pred == true_class)
        vals_correct = df.iloc[np.where(mask_correct_true)[0]][feature_to_plot].values

        plt.figure(figsize=(6, 4))
        plt.hist(vals_correct, bins=20, alpha=0.6, label=f"{true_class} correctly predicted")
        plt.hist(vals_mis, bins=20, alpha=0.6, label=f"{true_class}→{pred_class} misclassified")
        plt.xlabel(feature_to_plot)
        plt.ylabel("Count")
        plt.title(f"Feature Distribution – {feature_to_plot}")
        plt.legend()
        plt.tight_layout()
        plt.show()


def run_misclassification_analysis():
    # Classical models (XGB)
    results_gtzan, results_korean = run_classical_rq1()

    for dataset_name, res in [("GTZAN", results_gtzan), ("Korean", results_korean)]:
        print(f"\n====== Misclassification Analysis for {dataset_name} (XGB) ======")
        df = res["df"]
        y_true = res["y_test"]
        y_pred = res["y_pred_xgb"]
        cm = res["cm_xgb"]
        class_names = res["class_names"]

        # Map labels to indices
        class_to_idx = {c: i for i, c in enumerate(class_names)}
        cm_by_label = np.zeros_like(cm)
        # (if cm is already ordered by class_names, you can just reuse)

        confusion_pairs = get_confusion_pairs(cm, class_names, top_k=5)
        print("Top confusion pairs:", confusion_pairs)

        # Example: analyze first pair on some MFCC feature
        if confusion_pairs:
            (true_c, pred_c), count = confusion_pairs[0]
            # Pick a feature name to plot
            candidate_features = [f for f in res["feature_cols"] if "mfcc" in f.lower()]
            feature_to_plot = candidate_features[0] if candidate_features else res["feature_cols"][0]

            analyze_confusion_pair_features(
                df, y_true, y_pred,
                true_class=true_c, pred_class=pred_c,
                feature_cols=res["feature_cols"],
                feature_to_plot=feature_to_plot
            )
